{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ee3a911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\karim\\python_workspace\\pytorch_learning\\nlp-transformers\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2263f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = load_dataset('emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5699bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3ca1f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c182197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2023, 2003, 1037, 3231,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"this is a test\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45ef23db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82db21df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[-0.1565, -0.1862,  0.0528,  ..., -0.1188,  0.0662,  0.5470],\n",
      "         [-0.3575, -0.6484, -0.0618,  ..., -0.3040,  0.3508,  0.5221],\n",
      "         [-0.2772, -0.4459,  0.1818,  ..., -0.0948, -0.0076,  0.9958],\n",
      "         [-0.2841, -0.3917,  0.3753,  ..., -0.2151, -0.1173,  1.0526],\n",
      "         [ 0.2661, -0.5094, -0.3180,  ..., -0.4203,  0.0144, -0.2149],\n",
      "         [ 0.9441,  0.0112, -0.4714,  ...,  0.1439, -0.7288, -0.1619]]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs =  model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3783ce50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 768])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.last_hidden_state.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6509c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39aec14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1565, -0.1862,  0.0528,  ..., -0.1188,  0.0662,  0.5470],\n",
       "         [-0.3575, -0.6484, -0.0618,  ..., -0.3040,  0.3508,  0.5221],\n",
       "         [-0.2772, -0.4459,  0.1818,  ..., -0.0948, -0.0076,  0.9958],\n",
       "         [-0.2841, -0.3917,  0.3753,  ..., -0.2151, -0.1173,  1.0526],\n",
       "         [ 0.2661, -0.5094, -0.3180,  ..., -0.4203,  0.0144, -0.2149],\n",
       "         [ 0.9441,  0.0112, -0.4714,  ...,  0.1439, -0.7288, -0.1619]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4644119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c1b49e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the cls token \n",
    "hidden_states[:, 0, :].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62f81ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my task is to take a batch\n",
    "# get the hidden state for the CLS token\n",
    "\n",
    "# so how could we do it\n",
    "\n",
    "# first take in the batc so im expecting it to be smth like \n",
    "# [{input_id:tensor , input_mask:tensor},{input_id:tensor , input_mask:tensor}, ...... , {input_id:tensor , input_mask:tensor}]\n",
    "\n",
    "###\n",
    "# func extract_hidden_state(batch)\n",
    "# \n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dcf4b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [\n",
    "    \"this is sentence one\",\n",
    "    \"this is senntence two, lets make it uneven\", \n",
    "    \"this is the last sentence\"\n",
    "]\n",
    "inputs = tokenizer(batch , padding = True, truncation = True, return_tensors = \"pt\")\n",
    "inputs['input_ids'].size()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# shape is {input_ids: [[], [], .... []]}\n",
    "# so if input_ids is a tensor then its shape will be \n",
    "# assume n = batch_size, m = len_of_tokens_in_longest_input, and \n",
    "# so the shape of the tensor is (n , m) for both the input_ids and the input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2edac4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# im expecting a lsit of strings to be the batch\n",
    "\n",
    "def tokenize_input(batch , key):\n",
    "    return tokenizer(batch[key], padding = True , truncation= True , return_tensors=\"pt\")\n",
    "def tokenize_input_test(string_list):\n",
    "    return tokenizer(batch , padding = True , truncation = True , return_tensors = \"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c30a98f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_state(batch):\n",
    "    inputs = { k : v.to(device) for k , v in batch.items() if k in tokenizer.model_input_names }\n",
    "    print(inputs)\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "    \n",
    "    return last_hidden_state[: , 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7dd27d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_train = emotions['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbb680ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f195366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2023,  2003,  6251,  2028,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0],\n",
       "        [  101,  2023,  2003, 12411, 10111,  5897,  2048,  1010, 11082,  2191,\n",
       "          2009, 17837,   102],\n",
       "        [  101,  2023,  2003,  1996,  2197,  6251,   102,     0,     0,     0,\n",
       "             0,     0,     0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_list = [\n",
    "    \"this is input one \",\n",
    "    \"this is input two this sentence is longer\"\n",
    "    \"this is input three this sentence is much longer\"\n",
    "    \"this is input four this sentence is shorter\"\n",
    "]\n",
    "tokenized_input = tokenize_input_test(string_list)\n",
    "tokenized_input['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a3da61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2527d16c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cf48ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
